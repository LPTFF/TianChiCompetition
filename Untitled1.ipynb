{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2nRIGWVt7bJS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import pickle\n",
        "def mask_padding(arr):\n",
        "    mask=np.zeros((arr.shape))\n",
        "    mask[arr>0]=1\n",
        "    return torch.from_numpy(mask).float()\n",
        "\n",
        "\n",
        "class dataset(data.Dataset):\n",
        "  def __init__(self,path='arr.pkl',mask=mask_padding):\n",
        "    self.path=path\n",
        "    self.mask=mask_padding\n",
        "    with open('arr.pkl','rb') as fp:\n",
        "      self.arr=pickle.load(fp)\n",
        "  def __getitem__(self,index):\n",
        "    mask1=self.mask(self.arr[index])\n",
        "    length=(mask1>0).sum()\n",
        "    return torch.from_numpy(self.arr[index]).long(),torch.from_numpy(self.arr[index]).long(),torch.tensor(length.item()),mask1\n",
        "  def __len__(self):\n",
        "    return len(self.arr)\n",
        "  \n",
        "with open('word_embed.pkl','rb') as fp:\n",
        "  pretrain_vector=pickle.load(fp)\n",
        "\n",
        "with open('word2index.pkl','rb') as f:\n",
        "  word2index=pickle.load(f)\n",
        "pretrain_vector=torch.from_numpy(pretrain_vector).float()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIemjZ46yUJ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "seq2seq"
      ]
    },
    {
      "metadata": {
        "id": "99Lm9hEMyYxg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Seq2SeqRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, rnn_type,  hidden_size, embz_size,batch_size,\n",
        "                 attention_type, tied_weight_type, pre_trained_vector, \n",
        "                 num_layers=1, encoder_drop=(0.2,0.3), decoder_drop=(0.2,0.3), \n",
        "                 bidirectional=True, bias=False, teacher_forcing=True):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        rnn_type, attention_type, tied_weight_type = rnn_type.upper(), attention_type.title(), tied_weight_type.lower()\n",
        "        \n",
        "        if rnn_type in ['LSTM', 'GRU']: self.rnn_type = rnn_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--rnn_type' was supplied,\n",
        "                                    options are ['LSTM', 'GRU']\"\"\")\n",
        "            \n",
        "        if attention_type in ['Luong', 'Bahdanau']: self.attention_type = attention_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--attention_type' was supplied,\n",
        "                                    options are ['Luong', 'Bahdanau']\"\"\")\n",
        "            \n",
        "        if tied_weight_type in ['three_way', 'two_way']: self.tied_weight_type = tied_weight_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--tied_weight_type' was supplied,\n",
        "                                    options are ['three_way', 'two_way']\"\"\")\n",
        "    \n",
        "                    \n",
        "        #initialize model parameters            \n",
        "        self.embz_size, self.hidden_size =  embz_size, hidden_size//2\n",
        "        self.num_layers,   self.pre_trained_vector = num_layers,   pre_trained_vector\n",
        "        self.bidirectional,self.teacher_forcing = bidirectional, teacher_forcing\n",
        "        self.encoder_drop, self.decoder_drop = encoder_drop, decoder_drop\n",
        "        self.input_size=pre_trained_vector.size(0)\n",
        "        self.output_size=pre_trained_vector.size(0)\n",
        "        if self.teacher_forcing: self.force_prob = 0.5\n",
        "        \n",
        "        #set bidirectional\n",
        "        if self.bidirectional: self.num_directions = 2\n",
        "        else: self.num_directions = 1\n",
        "            \n",
        "        \n",
        "        #encoder\n",
        "        self.encoder_dropout = nn.Dropout(self.encoder_drop[0])\n",
        "        self.encoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size)\n",
        "        self.encoder_embedding_layer.weight.data.copy_(self.pre_trained_vector)\n",
        "            \n",
        "        self.encoder_rnn = getattr(nn, self.rnn_type)(\n",
        "                           input_size=self.embz_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers,\n",
        "                           dropout=self.encoder_drop[1], \n",
        "                           bidirectional=self.bidirectional,batch_first=True)\n",
        "        self.encoder_vector_layer = nn.Linear(self.hidden_size*self.num_directions,self.output_size, bias=bias)\n",
        "        \n",
        "       #decoder\n",
        "        self.decoder_dropout = nn.Dropout(self.decoder_drop[0])\n",
        "        self.decoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size)\n",
        "        self.decoder_rnn = getattr(nn, self.rnn_type)(\n",
        "                           input_size=self.embz_size,\n",
        "                           hidden_size=self.hidden_size*self.num_directions,\n",
        "                           num_layers=self.num_layers,\n",
        "                           dropout=self.decoder_drop[1],batch_first=True) \n",
        "        self.decoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "        self.output_layer = nn.Linear(self.embz_size, self.output_size, bias=bias)\n",
        "        \n",
        "        #set tied weights: three way tied weights vs two way tied weights\n",
        "        if self.tied_weight_type == 'three_way':\n",
        "            self.decoder_embedding_layer.weight  = self.encoder_embedding_layer.weight\n",
        "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
        "        else:\n",
        "            self.decoder_embedding_layer.weight.data.copy_(self.pre_trained_vector)\n",
        "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
        "            \n",
        "        #set attention\n",
        "        self.encoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "        self.att_vector_layer = nn.Linear(self.embz_size+self.embz_size, self.embz_size,bias=bias)\n",
        "        if self.attention_type == 'Bahdanau':\n",
        "            self.decoder_hidden_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "            self.att_score = nn.Linear(self.embz_size,1,bias=bias)\n",
        "\n",
        "            \n",
        "    \n",
        "    def init_hidden(self, batch_size=10):\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size),\n",
        "                    torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size))\n",
        "        else:\n",
        "            return torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size)\n",
        "   \n",
        "\n",
        "    def _cat_directions(self, hidden):\n",
        "        def _cat(h):\n",
        "            return torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
        "            \n",
        "        if isinstance(hidden, tuple):\n",
        "            # LSTM hidden contains a tuple (hidden state, cell state)\n",
        "            hidden = tuple([_cat(h) for h in hidden])\n",
        "        else:\n",
        "            # GRU hidden\n",
        "            hidden = _cat(hidden)\n",
        "        return hidden    \n",
        "    \n",
        "    \n",
        "    def bahdanau_attention(self, encoder_output, decoder_hidden, decoder_input):\n",
        "        encoder_output = self.encoder_output_layer(encoder_output) \n",
        "        encoder_output = encoder_output\n",
        "        decoder_hidden = decoder_hidden\n",
        "        att_score = torch.tanh(encoder_output + decoder_hidden)\n",
        "        att_score = self.att_score(att_score)\n",
        "        att_weight = F.softmax(att_score, dim=1)\n",
        "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
        "        att_vector = torch.cat((context_vector, decoder_input), dim=1)\n",
        "        att_vector = self.att_vector_layer(att_vector)\n",
        "        att_vector = torch.tanh(att_vector)\n",
        "        return att_weight.squeeze(-1), att_vector\n",
        "    \n",
        "    \n",
        "    def luong_attention(self, encoder_output, decoder_output):\n",
        "        encoder_output = self.encoder_output_layer(encoder_output) \n",
        "        encoder_output = encoder_output\n",
        "        decoder_output = decoder_output\n",
        "        \n",
        "        att_score = torch.bmm(encoder_output, decoder_output.transpose(-1,1))\n",
        "        att_weight = F.softmax(att_score, dim=1)\n",
        "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
        "        att_vector = torch.cat((context_vector, decoder_output.squeeze(1)), dim=1)\n",
        "        att_vector = self.att_vector_layer(att_vector)\n",
        "        att_vector = torch.tanh(att_vector)\n",
        "        return att_weight.squeeze(-1), att_vector\n",
        "        \n",
        "    def decoder_forward(self, batch_size, encoder_output, decoder_hidden, y,length):\n",
        "        decoder_input = torch.zeros(batch_size).long()\n",
        "        output_seq_stack, att_stack = [], []\n",
        "        \n",
        "        for i in range(max(length)):\n",
        "            decoder_input = self.decoder_dropout(self.decoder_embedding_layer(decoder_input))\n",
        "            if self.attention_type == 'Bahdanau':\n",
        "                if isinstance(decoder_hidden, tuple):\n",
        "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[0][-1]).unsqueeze(0)\n",
        "                else:\n",
        "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[-1]).unsqueeze(0) \n",
        "                att, decoder_input = self.bahdanau_attention(encoder_output, prev_hidden, decoder_input)\n",
        "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(1), decoder_hidden)\n",
        "                decoder_output = self.decoder_output_layer(decoder_output.squeeze(1)) \n",
        "            else:\n",
        "\n",
        "                #print(decoder_hidden.size(),decoder_input.unsqueeze(1).size())\n",
        "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(1), decoder_hidden)\n",
        "                decoder_output = self.decoder_output_layer(decoder_output) \n",
        "                att, decoder_output = self.luong_attention(encoder_output, decoder_output)\n",
        "            att_stack.append(att)\n",
        "            output = self.output_layer(decoder_output)\n",
        "            output_seq_stack.append(output)\n",
        "            \n",
        "            #decoder_input = V(output.data.max(1)[1])\n",
        "            \n",
        "            \n",
        "            if self.teacher_forcing:    \n",
        "               \n",
        "                if (y is not None):\n",
        "                  decoder_input=y[:,i].long()\n",
        "                     \n",
        "                \n",
        "        return torch.stack(output_seq_stack), torch.stack(att_stack)\n",
        "        \n",
        "                \n",
        "    def forward(self, seq, y,length):\n",
        "        batch_size = seq.size(0)\n",
        "        \n",
        "        encoder_hidden = self.init_hidden(batch_size)\n",
        "        encoder_input = self.encoder_dropout(self.encoder_embedding_layer(seq))\n",
        "       \n",
        "        encoder_packed = torch.nn.utils.rnn.pack_padded_sequence(encoder_input, length, batch_first=True)\n",
        "        encoder_output, encoder_hidden = self.encoder_rnn(encoder_packed, encoder_hidden) \n",
        "        encoder_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(encoder_output, batch_first=True)\n",
        "        encoder_outputs=encoder_outputs\n",
        "        if self.bidirectional:\n",
        "            encoder_hidden = self._cat_directions(encoder_hidden)\n",
        "        output,_ = self.decoder_forward(batch_size, encoder_outputs, encoder_hidden, y=y,length=length)\n",
        "        if isinstance(encoder_hidden, tuple):\n",
        "            encoder_vector = self.encoder_vector_layer(encoder_hidden[0][-1])\n",
        "        else:\n",
        "            encoder_vector = self.encoder_vector_layer(encoder_hidden[-1])\n",
        "        output = output + (encoder_vector.unsqueeze(0))  \n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4XOfmfiBCMA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "09cf8492-8339-4f52-ec93-b1a8e71635af"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([2, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x0BdlAmJ5P4d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437
        },
        "outputId": "97a91b26-2898-4b7b-bde3-3879dd476036"
      },
      "cell_type": "code",
      "source": [
        "index2word={}\n",
        "for x,y in word2index.items():\n",
        "  index2word[y]=x\n",
        "  \n",
        "def print_sentence(pre):\n",
        "  if len(pre.size())==3:\n",
        "    _,a1=pre.max(2)\n",
        "  else:\n",
        "    a1=pre\n",
        "  a1=a1.numpy()\n",
        "  batch_1=a1[:,0]\n",
        "  b1=[]\n",
        "  for i in batch_1:\n",
        "    if i in index2word:\n",
        "      b1.append(index2word[i])\n",
        "    else:\n",
        "      b1.append('unknow')\n",
        "  return ' '.join(b1)\n",
        "      \n",
        "\n",
        "model=Seq2SeqRNN('LSTM',400,300,10,'Luong','two_way', pretrain_vector)\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "sentence=dataset()\n",
        "datasets=data.DataLoader(sentence,batch_size=10,shuffle=True)\n",
        "for j in range(100):\n",
        "  for i,(x,y,z,m) in enumerate(datasets):\n",
        "    a,b=z.sort(descending=True)\n",
        "    x=x[b]\n",
        "  \n",
        "    y=y[b]\n",
        "    z=a\n",
        "    max_l=max(a)\n",
        "    m=m[b]\n",
        "    out=model(x,y,z)\n",
        "    out1=F.log_softmax(out,dim=2)\n",
        "    y=y[:,:max_l]\n",
        "    m=m[:,:max_l]\n",
        "    m=m.permute(1,0).view(m.size(1),m.size(0),1)\n",
        "    y=y.permute(1,0).view(y.size(1),y.size(0),1)\n",
        "    output_loss=torch.gather(out1,2,y)\n",
        "    \n",
        "    total_loss=-(1/(len(a)))*(output_loss*m).sum()\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(),3)\n",
        "    if i%100==0:\n",
        "      \n",
        "      print('loss',total_loss.data)\n",
        "      pre=print_sentence(out)\n",
        "      rea=print_sentence(y.squeeze(2))\n",
        "      print(pre,'###',rea)\n",
        "torch.save(model.state_dict(),'train:epoach{}.pth'.format(j)) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:46: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loss tensor(80.9952)\n",
            "Javascript gravatar JavaFX Javascript Oracle graphing graphing renaming physical physical scheduling Sticky Sticky physical coefficient media ### Oracle connection Mac OSX: \"Status : Failure -Test failed: Io exception: The Network Adapter establish connection\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-c407ba7068dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mamsgrad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0;31m# Maintains the maximum of all 2nd moment running avg. till now\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}