{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "2nRIGWVt7bJS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data as data\n",
        "import pickle\n",
        "def mask_padding(arr):\n",
        "    mask=np.zeros((arr.shape))\n",
        "    mask[arr>0]=1\n",
        "    return torch.from_numpy(mask).float()\n",
        "\n",
        "\n",
        "class dataset(data.Dataset):\n",
        "  def __init__(self,path='arr.pkl',mask=mask_padding):\n",
        "    self.path=path\n",
        "    self.mask=mask_padding\n",
        "    with open('arr.pkl','rb') as fp:\n",
        "      self.arr=pickle.load(fp)\n",
        "  def __getitem__(self,index):\n",
        "    mask1=self.mask(self.arr[index])\n",
        "    length=(mask1>0).sum()\n",
        "    return torch.from_numpy(self.arr[index]).long(),torch.from_numpy(self.arr[index]).long(),torch.tensor(length.item()),mask1\n",
        "  def __len__(self):\n",
        "    return len(self.arr)\n",
        "  \n",
        "with open('word_embed.pkl','rb') as fp:\n",
        "  pretrain_vector=pickle.load(fp)\n",
        "\n",
        "with open('word2index.pkl','rb') as f:\n",
        "  word2index=pickle.load(f)\n",
        "pretrain_vector=torch.from_numpy(pretrain_vector).float()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIemjZ46yUJ-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "seq2seq"
      ]
    },
    {
      "metadata": {
        "id": "99Lm9hEMyYxg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Seq2SeqRNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, rnn_type,  hidden_size, embz_size,batch_size,\n",
        "                 attention_type, tied_weight_type, pre_trained_vector, \n",
        "                 num_layers=1, encoder_drop=(0.2,0.3), decoder_drop=(0.2,0.3), \n",
        "                 bidirectional=True, bias=False, teacher_forcing=True):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        rnn_type, attention_type, tied_weight_type = rnn_type.upper(), attention_type.title(), tied_weight_type.lower()\n",
        "        \n",
        "        if rnn_type in ['LSTM', 'GRU']: self.rnn_type = rnn_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--rnn_type' was supplied,\n",
        "                                    options are ['LSTM', 'GRU']\"\"\")\n",
        "            \n",
        "        if attention_type in ['Luong', 'Bahdanau']: self.attention_type = attention_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--attention_type' was supplied,\n",
        "                                    options are ['Luong', 'Bahdanau']\"\"\")\n",
        "            \n",
        "        if tied_weight_type in ['three_way', 'two_way']: self.tied_weight_type = tied_weight_type\n",
        "        else: raise ValueError(\"\"\"An invalid option for '--tied_weight_type' was supplied,\n",
        "                                    options are ['three_way', 'two_way']\"\"\")\n",
        "    \n",
        "                    \n",
        "        #initialize model parameters            \n",
        "        self.embz_size, self.hidden_size =  embz_size, hidden_size//2\n",
        "        self.num_layers,   self.pre_trained_vector = num_layers,   pre_trained_vector\n",
        "        self.bidirectional,self.teacher_forcing = bidirectional, teacher_forcing\n",
        "        self.encoder_drop, self.decoder_drop = encoder_drop, decoder_drop\n",
        "        self.input_size=pre_trained_vector.size(0)\n",
        "        self.output_size=pre_trained_vector.size(0)\n",
        "        if self.teacher_forcing: self.force_prob = 0.5\n",
        "        \n",
        "        #set bidirectional\n",
        "        if self.bidirectional: self.num_directions = 2\n",
        "        else: self.num_directions = 1\n",
        "            \n",
        "        \n",
        "        #encoder\n",
        "        self.encoder_dropout = nn.Dropout(self.encoder_drop[0])\n",
        "        self.encoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size)\n",
        "        self.encoder_embedding_layer.weight.data.copy_(self.pre_trained_vector)\n",
        "            \n",
        "        self.encoder_rnn = getattr(nn, self.rnn_type)(\n",
        "                           input_size=self.embz_size,\n",
        "                           hidden_size=self.hidden_size,\n",
        "                           num_layers=self.num_layers,\n",
        "                           dropout=self.encoder_drop[1], \n",
        "                           bidirectional=self.bidirectional,batch_first=True)\n",
        "        self.encoder_vector_layer = nn.Linear(self.hidden_size*self.num_directions,self.output_size, bias=bias)\n",
        "        \n",
        "       #decoder\n",
        "        self.decoder_dropout = nn.Dropout(self.decoder_drop[0])\n",
        "        self.decoder_embedding_layer = nn.Embedding(self.input_size, self.embz_size)\n",
        "        self.decoder_rnn = getattr(nn, self.rnn_type)(\n",
        "                           input_size=self.embz_size,\n",
        "                           hidden_size=self.hidden_size*self.num_directions,\n",
        "                           num_layers=self.num_layers,\n",
        "                           dropout=self.decoder_drop[1],batch_first=True) \n",
        "        self.decoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "        self.output_layer = nn.Linear(self.embz_size, self.output_size, bias=bias)\n",
        "        \n",
        "        #set tied weights: three way tied weights vs two way tied weights\n",
        "        if self.tied_weight_type == 'three_way':\n",
        "            self.decoder_embedding_layer.weight  = self.encoder_embedding_layer.weight\n",
        "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
        "        else:\n",
        "            self.decoder_embedding_layer.weight.data.copy_(self.pre_trained_vector)\n",
        "            self.output_layer.weight = self.decoder_embedding_layer.weight  \n",
        "            \n",
        "        #set attention\n",
        "        self.encoder_output_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "        self.att_vector_layer = nn.Linear(self.embz_size+self.embz_size, self.embz_size,bias=bias)\n",
        "        if self.attention_type == 'Bahdanau':\n",
        "            self.decoder_hidden_layer = nn.Linear(self.hidden_size*self.num_directions, self.embz_size, bias=bias)\n",
        "            self.att_score = nn.Linear(self.embz_size,1,bias=bias)\n",
        "\n",
        "            \n",
        "####################################   \n",
        "    def init_hidden(self, batch_size=10):\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size).cuda(),\n",
        "                    torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size).cuda())\n",
        "        else:\n",
        "            return torch.zeros(self.num_layers*self.num_directions, batch_size, self.hidden_size).cuda()\n",
        "##########################################   \n",
        "\n",
        "    def _cat_directions(self, hidden):\n",
        "        def _cat(h):\n",
        "            return torch.cat([h[0:h.size(0):2], h[1:h.size(0):2]], 2)\n",
        "            \n",
        "        if isinstance(hidden, tuple):\n",
        "            # LSTM hidden contains a tuple (hidden state, cell state)\n",
        "            hidden = tuple([_cat(h) for h in hidden])\n",
        "        else:\n",
        "            # GRU hidden\n",
        "            hidden = _cat(hidden)\n",
        "        return hidden    \n",
        "    \n",
        "    \n",
        "    def bahdanau_attention(self, encoder_output, decoder_hidden, decoder_input):\n",
        "        encoder_output = self.encoder_output_layer(encoder_output) \n",
        "        encoder_output = encoder_output\n",
        "        decoder_hidden = decoder_hidden\n",
        "        att_score = torch.tanh(encoder_output + decoder_hidden)\n",
        "        att_score = self.att_score(att_score)\n",
        "        att_weight = F.softmax(att_score, dim=1)\n",
        "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
        "        att_vector = torch.cat((context_vector, decoder_input), dim=1)\n",
        "        att_vector = self.att_vector_layer(att_vector)\n",
        "        att_vector = torch.tanh(att_vector)\n",
        "        return att_weight.squeeze(-1), att_vector\n",
        "    \n",
        "    \n",
        "    def luong_attention(self, encoder_output, decoder_output):\n",
        "        encoder_output = self.encoder_output_layer(encoder_output) \n",
        "        encoder_output = encoder_output\n",
        "        decoder_output = decoder_output\n",
        "        \n",
        "        att_score = torch.bmm(encoder_output, decoder_output.transpose(-1,1))\n",
        "        att_weight = F.softmax(att_score, dim=1)\n",
        "        context_vector = torch.bmm(att_weight.transpose(-1, 1), encoder_output).squeeze(1)\n",
        "        att_vector = torch.cat((context_vector, decoder_output.squeeze(1)), dim=1)\n",
        "        att_vector = self.att_vector_layer(att_vector)\n",
        "        att_vector = torch.tanh(att_vector)\n",
        "        return att_weight.squeeze(-1), att_vector\n",
        "        \n",
        "    def decoder_forward(self, batch_size, encoder_output, decoder_hidden, y,length):\n",
        "       ############################################\n",
        "        decoder_input = torch.zeros(batch_size).long().cuda()\n",
        "        output_seq_stack, att_stack = [], []\n",
        "        \n",
        "        for i in range(max(length)):\n",
        "            decoder_input = self.decoder_dropout(self.decoder_embedding_layer(decoder_input))\n",
        "            if self.attention_type == 'Bahdanau':\n",
        "                if isinstance(decoder_hidden, tuple):\n",
        "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[0][-1]).unsqueeze(0)\n",
        "                else:\n",
        "                    prev_hidden = self.decoder_hidden_layer(decoder_hidden[-1]).unsqueeze(0) \n",
        "                att, decoder_input = self.bahdanau_attention(encoder_output, prev_hidden, decoder_input)\n",
        "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(1), decoder_hidden)\n",
        "                decoder_output = self.decoder_output_layer(decoder_output.squeeze(1)) \n",
        "            else:\n",
        "\n",
        "                #print(decoder_hidden.size(),decoder_input.unsqueeze(1).size())\n",
        "                decoder_output, decoder_hidden = self.decoder_rnn(decoder_input.unsqueeze(1), decoder_hidden)\n",
        "                decoder_output = self.decoder_output_layer(decoder_output) \n",
        "                att, decoder_output = self.luong_attention(encoder_output, decoder_output)\n",
        "            att_stack.append(att)\n",
        "            output = self.output_layer(decoder_output)\n",
        "            output_seq_stack.append(output)\n",
        "            \n",
        "            #decoder_input = V(output.data.max(1)[1])\n",
        "            \n",
        "            \n",
        "            if self.teacher_forcing:    \n",
        "               \n",
        "                if (y is not None):\n",
        "                  decoder_input=y[:,i].long()\n",
        "                     \n",
        "                \n",
        "        return torch.stack(output_seq_stack), torch.stack(att_stack)\n",
        "        \n",
        "                \n",
        "    def forward(self, seq, y,length):\n",
        "        batch_size = seq.size(0)\n",
        "        \n",
        "        encoder_hidden = self.init_hidden(batch_size)\n",
        "        encoder_input = self.encoder_dropout(self.encoder_embedding_layer(seq))\n",
        "       \n",
        "        encoder_packed = torch.nn.utils.rnn.pack_padded_sequence(encoder_input, length, batch_first=True)\n",
        "        encoder_output, encoder_hidden = self.encoder_rnn(encoder_packed, encoder_hidden) \n",
        "        encoder_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(encoder_output, batch_first=True)\n",
        "        encoder_outputs=encoder_outputs\n",
        "        if self.bidirectional:\n",
        "            encoder_hidden = self._cat_directions(encoder_hidden)\n",
        "        output,_ = self.decoder_forward(batch_size, encoder_outputs, encoder_hidden, y=y,length=length)\n",
        "        if isinstance(encoder_hidden, tuple):\n",
        "            encoder_vector = self.encoder_vector_layer(encoder_hidden[0][-1])\n",
        "        else:\n",
        "            encoder_vector = self.encoder_vector_layer(encoder_hidden[-1])\n",
        "        #output = output + (encoder_vector.unsqueeze(0))  \n",
        "        return output ,encoder_hidden[0][-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C4XOfmfiBCMA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x0BdlAmJ5P4d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "index2word={}\n",
        "for x,y in word2index.items():\n",
        "  index2word[y]=x\n",
        "  \n",
        "def print_sentence(pre):\n",
        "  if len(pre.size())==3:\n",
        "    _,a1=pre.max(2)\n",
        "  else:\n",
        "    a1=pre\n",
        "  a1=a1.cpu().numpy()\n",
        "  batch_1=a1[:,0]\n",
        "  b1=[]\n",
        "  for i in batch_1:\n",
        "    if i in index2word:\n",
        "      b1.append(index2word[i])\n",
        "    else:\n",
        "      b1.append('unknow')\n",
        "  return ' '.join(b1)\n",
        "      \n",
        "\n",
        "model=Seq2SeqRNN('LSTM',400,300,10,'Luong','two_way', pretrain_vector).cuda()\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n",
        "sentence=dataset()\n",
        "datasets=data.DataLoader(sentence,batch_size=10,shuffle=True)\n",
        "for j in range(10):\n",
        "  for i,(x,y,z,m) in enumerate(datasets):\n",
        "    a,b=z.sort(descending=True)\n",
        "    x=x[b].cuda()\n",
        "  \n",
        "    y=y[b].cuda()\n",
        "    z=a.cuda()\n",
        "    max_l=max(a)\n",
        "    m=m[b].cuda()\n",
        "    out,_=model(x,y,z)\n",
        "    out1=F.log_softmax(out,dim=2)\n",
        "    y=y[:,:max_l]\n",
        "    m=m[:,:max_l]\n",
        "    m=m.permute(1,0).view(m.size(1),m.size(0),1)\n",
        "    y=y.permute(1,0).view(y.size(1),y.size(0),1)\n",
        "    output_loss=torch.gather(out1,2,y)\n",
        "    \n",
        "    total_loss=-(1/(len(a)))*(output_loss*m).sum()\n",
        "    optimizer.zero_grad()\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    nn.utils.clip_grad_norm_(model.parameters(),3)\n",
        "    if i%500==0:\n",
        "      \n",
        "      print('loss',total_loss.data)\n",
        "      pre=print_sentence(out)\n",
        "      rea=print_sentence(y.squeeze(2))\n",
        "      print(pre,'###',rea)\n",
        "torch.save(model.cpu().state_dict(),'train:epoach{}.pth'.format(j))\n",
        "'''with torch.no_grad():\n",
        "  model=Seq2SeqRNN('LSTM',400,300,10,'Luong','two_way', pretrain_vector)\n",
        "  model.load_state_dict(torch.load('train:epoach9.pth'))\n",
        "  sentence=dataset()\n",
        "  \n",
        "  datasets=data.DataLoader(sentence,batch_size=1000,shuffle=False)\n",
        "  simple=[]\n",
        "  for i,(x,y,z,m) in enumerate(datasets):\n",
        "    a,b=z.sort(descending=True)\n",
        "    x=x[b]\n",
        "  \n",
        "    y=y[b]\n",
        "    z=a\n",
        "    max_l=max(a)\n",
        "    m=m[b]\n",
        "    out1,out=model(x,y,z)\n",
        "    simple.append(out.numpy())\n",
        "\n",
        "\n",
        "simple=np.vstack(simple)  \n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from operator import itemgetter\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "\n",
        "def map_label(true_labels, pred_labels):\n",
        "    label_pair = list(zip(pred_labels, true_labels))\n",
        "    count = tuple(Counter(label_pair).items())\n",
        "    mapping = dict()\n",
        "    n_label = len(np.unique(true_labels))\n",
        "\n",
        "    # map most likely labels from prediction to ground truth\n",
        "    for label in range(n_label):\n",
        "        tuples = [tup for tup in count if tup[0][0] == label]\n",
        "        likely_tuple = max(tuples, key=itemgetter(1))[0]\n",
        "        mapping[likely_tuple[0]] = likely_tuple[1]\n",
        "\n",
        "    pred_labels_mapped = [mapping[x] for x in pred_labels]\n",
        "    return pred_labels_mapped\n",
        "\n",
        "\n",
        "def cluster_quality(true_labels, pred_labels, show=True):\n",
        "    h, c, v = metrics.homogeneity_completeness_v_measure(true_labels, pred_labels)\n",
        "    nmi = metrics.normalized_mutual_info_score(true_labels, pred_labels)\n",
        "    rand = metrics.adjusted_rand_score(true_labels, pred_labels)\n",
        "    pred_labels_mapped = map_label(true_labels, pred_labels)\n",
        "    acc = metrics.accuracy_score(true_labels, pred_labels_mapped)\n",
        "    if show:\n",
        "        print(\"Homogeneity: %0.3f\" % h)\n",
        "        print(\"Completeness: %0.3f\" % c)\n",
        "        print(\"V-measure: %0.3f\" % v)\n",
        "        print(\"NMI: %0.3f\" % nmi)\n",
        "        print(\"Rand score: %0.3f\" % rand)\n",
        "        print(\"Accuracy: %0.3f\" % acc)\n",
        "    return dict(\n",
        "        homogeneity=h,\n",
        "        completeness=c,\n",
        "        vmeasure=v,\n",
        "        nmi=nmi,\n",
        "        rand=rand,\n",
        "        accuracy=acc,\n",
        "    )\n",
        "y=[]\n",
        "with open('target.txt','r') as f:\n",
        "    for i in f.readlines():\n",
        "        y.append(int(i))\n",
        "    \n",
        "true_labels = np.array(y)\n",
        "n_clusters = len(np.unique(y))\n",
        "print(\"Number of classes: %d\" % n_clusters)\n",
        "km = KMeans(n_clusters=n_clusters, n_jobs=20)\n",
        "result = dict()\n",
        "#input2 = normalize(embed, norm='l2')\n",
        "km.fit(simple)\n",
        "pred = km.labels_\n",
        "print(pred)\n",
        "a = {'deep': cluster_quality(true_labels, pred)}'''\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "s1O7K60C_rSa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}